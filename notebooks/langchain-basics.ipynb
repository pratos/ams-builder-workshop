{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50434934-907c-41dd-815c-5bf7b821b2ea",
   "metadata": {},
   "source": [
    "## Why LangChain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4f60c8-7c7a-437f-a2a0-396abddc98fa",
   "metadata": {},
   "source": [
    "There are times when we blindly accept things presented to us. We all have accepted that `LangChain` is the thing we NEED to use to do anything related to Large Language Models (LLMs). But why `LangChain` is the first question we want to answer in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977aaf99-9e04-4fd2-a7e7-f2d5eb937bd7",
   "metadata": {},
   "source": [
    "### Working with OpenAI's APIs without LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6431aa-f390-45b8-abcf-684c5f5eedaa",
   "metadata": {},
   "source": [
    "Let's pick the most popular LLMs in the market, OpenAI. Good folks @ OpenAI have provided a nice python wrapper (`pip install openai`) to their REST endpoints ([link here](https://platform.openai.com/docs/api-reference)). Without `LangChain`, we could work with the models provided easily. Let's see some examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0c1cc9d-bfee-4e4c-9634-0b6cfd718c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Add your openai key: ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "import openai\n",
    "openai.api_key = getpass(prompt=\"Add your openai key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef210678-d14e-44bf-97ae-51af21a1ee59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['whisper-1',\n",
       " 'babbage',\n",
       " 'davinci',\n",
       " 'text-davinci-edit-001',\n",
       " 'babbage-code-search-code']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = openai.Model.list()\n",
    "[model[\"id\"] for model in models[\"data\"][:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c873eb-8762-4d84-a9af-dd9f726d4d1a",
   "metadata": {},
   "source": [
    "Once we have set the keys, let's do a basic completion task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a49fca50-fa52-4301-a3b3-c6b3b68369cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Can you tell me who's the president of the United States of America?\"\n",
    "completion = openai.Completion.create(model=\"text-davinci-003\", prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c12e7d03-29df-4057-ac72-594f89ebefd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-7OXVg6lSFnH9Y2oxmKntB543rX5M1 at 0x117e568e0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \"\\n\\nThe current president of the United States is Joe Biden.\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1686083040,\n",
       "  \"id\": \"cmpl-7OXVg6lSFnH9Y2oxmKntB543rX5M1\",\n",
       "  \"model\": \"text-davinci-003\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 13,\n",
       "    \"prompt_tokens\": 15,\n",
       "    \"total_tokens\": 28\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7601d383-a40d-4281-992a-27b1eb8a3418",
   "metadata": {},
   "source": [
    "Cleaning up the data, we get the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d5c23e7-f912-4725-af8d-481642efe4ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe current President of the United States of America is Joe Biden.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion[\"choices\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35ae6b9-92d7-40ea-9cf5-aa089ab15b84",
   "metadata": {},
   "source": [
    "If we wanted to work with the latest 3.5 turbo/GPT-4 model, it needs different prompt which is compatible to the chat interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff7d5f7d-38ce-49f2-8996-6ee9b7fe3cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7OXVm8IM6fEl5loIl1O2ctwu7HDCq at 0x117cf8900> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"That's a great idea! Let's start with the days of the week. In Dutch, the days of the week are:\\n\\n- maandag (Monday)\\n- dinsdag (Tuesday)\\n- woensdag (Wednesday)\\n- donderdag (Thursday)\\n- vrijdag (Friday)\\n- zaterdag (Saturday)\\n- zondag (Sunday)\\n\\nCan you try to pronounce them after me?\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1686083046,\n",
       "  \"id\": \"chatcmpl-7OXVm8IM6fEl5loIl1O2ctwu7HDCq\",\n",
       "  \"model\": \"gpt-3.5-turbo-0301\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 83,\n",
       "    \"prompt_tokens\": 48,\n",
       "    \"total_tokens\": 131\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a Dutch language teacher who helps newbies learn Dutch faster. Please converse with the user as a new learner\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What would be our first learning? Week of the days?\"\n",
    "    }\n",
    "]\n",
    "completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c995a675-11a4-4b03-9754-381f2c8ae8f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's a great idea! Let's start with the days of the week. In Dutch, the days of the week are:\\n\\n- maandag (Monday)\\n- dinsdag (Tuesday)\\n- woensdag (Wednesday)\\n- donderdag (Thursday)\\n- vrijdag (Friday)\\n- zaterdag (Saturday)\\n- zondag (Sunday)\\n\\nCan you try to pronounce them after me?\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069ca091-7d84-4054-b540-c10e88166847",
   "metadata": {},
   "source": [
    "Now if I've to continue the conversation, I'd have to do a few things: \n",
    "1. Save the latest response and append it to `messages`\n",
    "```python\n",
    "messages = messages + [{\"role\": \"assistant\", \"content\": completion[\"choices\"][0][\"message\"][\"content\"]}]\n",
    "```\n",
    "2. Call the same `openai.ChatCompletion.create` function and send them back\n",
    "3. Rinse and repeat until I exhaust my `2k` context window for `3.5-turbo` and `4k` context window for `gpt-4`\n",
    "\n",
    "2k and 4k context windows are large, but they also cost a lot when you send each query back. How do I track what's the size of my context window everytime I call openai? Use `tiktoken`, which lets you know how much tokens are you sending to openai for a specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69c98a74-b3a3-42a4-8367-1ae2ff7d97c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0408854d-d949-4325-a7d0-29b10c24144a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2028, 374, 264, 1695, 2035]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_message = enc.encode(\"This is a good place\")\n",
    "encoded_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ade0844-b7ec-4e54-8323-01b804e47f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a good place'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.decode(encoded_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7fd80c8f-98b1-42f2-8bfd-1c89ad81eae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total tokens for gpt3.5-turbo --> 5'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Total tokens for gpt3.5-turbo --> {len([enc.decode_single_token_bytes(token) for token in encoded_message])}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "07896cb2-e8f8-40ff-8f59-4ba54a8bc15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens for latest message: 48\n"
     ]
    }
   ],
   "source": [
    "# Shameless copy-pasta from OpenAI example\n",
    "num_tokens = 0\n",
    "tokens_per_message = 4 # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "tokens_per_name = -1 # if there's a name, the role is omitted\n",
    "for message in messages:\n",
    "    num_tokens += tokens_per_message\n",
    "    for key, value in message.items():\n",
    "        num_tokens += len(enc.encode(value))\n",
    "        if key == \"name\":\n",
    "            num_tokens += tokens_per_name\n",
    "num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "print(f\"Number of tokens for latest message: {num_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7c99c455-3063-46fc-a2b3-b38b37f42f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert completion[\"usage\"][\"prompt_tokens\"] == num_tokens, \"Wrong implementation\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9a28064-db78-404a-ae40-5e9c9fb1d777",
   "metadata": {},
   "source": [
    "Our assert succeeds, but that's still a lot of work! Just to do a basic query. For a fairly robust implementation, we would need a few things:\n",
    "- Retries, OpenAI APIs are notoriously unstable with queries getting a lot of timeouts\n",
    "- Caching, You don't want to waste considerable energy to generate a completion for similar query by same/another user\n",
    "- Stardardized output schema, If your use-case demands a standardized output which could be a json/xml schema you need to invest in all those things.\n",
    "\n",
    "The above are just basic tasks that we have just mentioned. Phew!\n",
    "![tired-meme](https://i.kym-cdn.com/entries/icons/original/000/039/399/ddw.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23495406-ba25-4dac-aa9c-b2af40c4500a",
   "metadata": {},
   "source": [
    "Also, there's OpenAI but other alternatives like Cohere, Anthropic, Falcon, Llama that everyone would want to at least try out if not use in production. Models like Anthropic's `Claude-Instant-v1` literally blows OpenAI's `gpt3.5-turbo` out of the water ([read here](https://twitter.com/vladquant/status/1659679709154934784))\n",
    "\n",
    "As we mentioned above; working with LLMs, any engineer/product person would need the ability to iterate fast and have multiple options to try out. `LangChain` is THAT library right now. All the right (almost) abstractions required for LLMs are baked in `LangChain`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260d817c-59d5-4592-8796-70701f7ea153",
   "metadata": {},
   "source": [
    "### Working with OpenAI's APIs with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bd18d7-07a2-4d63-81e9-24745cd8fcd7",
   "metadata": {},
   "source": [
    "`LangChain` provides `llms` as the basic construct, helping us to easily swap between models (local and 3rd party)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11d0aa1-3553-4eca-b6e1-cda2cdb6b2a9",
   "metadata": {},
   "source": [
    "Let's first try out `OpenAI` wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5589b3f1-95e3-44a4-90c0-3b8317e64262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0d1a6e6a-1dd3-4eb4-87d6-32459799e4f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcache\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCallbackHandler\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCallbackManager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallback_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCallbackManager\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mclient\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'text-davinci-003'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtemperature\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtop_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfrequency_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpresence_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbest_of\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mopenai_api_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mopenai_api_base\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mopenai_organization\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mopenai_proxy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlogit_bias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_retries\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstreaming\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mallowed_special\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAbstractSet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdisallowed_special\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCollection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Wrapper around OpenAI large language models.\n",
       "\n",
       "To use, you should have the ``openai`` python package installed, and the\n",
       "environment variable ``OPENAI_API_KEY`` set with your API key.\n",
       "\n",
       "Any parameters that are valid to be passed to the openai.create call can be passed\n",
       "in, even if not explicitly saved on this class.\n",
       "\n",
       "Example:\n",
       "    .. code-block:: python\n",
       "\n",
       "        from langchain.llms import OpenAI\n",
       "        openai = OpenAI(model_name=\"text-davinci-003\")\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Create a new model by parsing and validating input data from keyword arguments.\n",
       "\n",
       "Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/personal-projects/ams-builder-workshop/.venv/lib/python3.10/site-packages/langchain/llms/openai.py\n",
       "\u001b[0;31mType:\u001b[0m           ModelMetaclass\n",
       "\u001b[0;31mSubclasses:\u001b[0m     PromptLayerOpenAI"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8044815b-0f5e-4c93-ab28-2fca866370c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt35_turbo = OpenAI(model_name='text-davinci-003', openai_api_key=open_ai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bff07567-714c-4656-b0ca-b489572aff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation = gpt35_turbo.generate(prompts=[prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b731a073-6c77-4e3b-b0a0-6a76b329dc4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Generation(text='\\n\\nThe President of the United States of America is Joe Biden.', generation_info={'finish_reason': 'stop', 'logprobs': None})]]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation.generations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096f1ce3-fba8-44a5-bc6b-66357b470654",
   "metadata": {},
   "source": [
    "One could even do a variation of the above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b0835a1e-b0db-42f0-83a6-b0b42db09ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe President of the United States of America is Joe Biden.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation.generations[0][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7f3109c8-777e-4044-b65c-0de42da4fd52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe current president of the United States of America is Joe Biden.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt35_turbo(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abf68ea-5eec-48a0-94ed-05c5f8665f32",
   "metadata": {},
   "source": [
    "Simple and Carefree outputs, without parsing through the json outputs that openai provides. Is that it? Nope.\n",
    "\n",
    "#### Swap 3rd party to local models \n",
    "Let's swap OpenAI for a fairly small local model: `flan-t5` from google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c7253769-9f06-4c95-be8d-c2e004f22dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e2aaad0e-5b2f-466a-828c-7d2c3c1991e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Add huggingface token (Visit -> https://huggingface.co/settings/tokens): ········\n"
     ]
    }
   ],
   "source": [
    "hf_token = getpass(prompt=\"Add huggingface token (Visit -> https://huggingface.co/settings/tokens):\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e655a2c1-9efc-420a-b54d-56119b267688",
   "metadata": {},
   "source": [
    "One can search for models here: [huggingface models](https://huggingface.co/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "16b499d0-56cc-4eef-ba2e-2afb7985a21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "flan_t5 = HuggingFaceHub(repo_id=\"google/flan-t5-small\", huggingfacehub_api_token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "60473ed7-3103-4189-a63b-c20d03729d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'John F. Kennedy'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flan_t5(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d7c53f-43f9-45f5-b0fb-5a6078262689",
   "metadata": {},
   "source": [
    "Ugggh! It is a fairly bad model, `flan-t5-xxl` might be a better one yet `OpenAI` models triumph the rest. At least we are sure that these models would be available for us if we need local inference or our use-cases are for sensitive data.\n",
    "\n",
    "__NOTE__: \n",
    "- Since Meta \"released\" Llama weights, there's a been an unending procession of very OpenAI compareable models (Vicuna-13B, Falcon come to mind). But those need a bunch of compute to run off locally or even on platforms like Replicate. So the next time you think about running these models, a Mac M1 Air or even a 3060RTX might not be able to run these due to hardware constraints.\n",
    "- Not to digress, but there's a class of quantized models released by ggml.ai that run on M1s/M2s at least. More improvement are coming in, but they are still worse off that OpenAI/Anthropic/Cohere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f94825-e313-4f95-a24b-ec70fc8ae5ca",
   "metadata": {},
   "source": [
    "#### Writing complex prompts with dynamic information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2433d2f4-1599-40c6-8252-eb19971a8043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Can you tell me who's the president of the United States of America?\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd543b5-0097-4da6-91a1-aaae4e98d943",
   "metadata": {},
   "source": [
    "The `prompt` above is the basic-est example that one can throw at an LLM. In the AI Summer before the cambrian explosion of LLMs, one had to pain-stakingly create models specific to a task.\n",
    "\n",
    "Want to do English to Dutch translation? Train a model\n",
    "Want to do nlp classification? Train a model\n",
    "\n",
    "LLMs kind of let you cheat your way through by just using one model. __One model to rule them all__\n",
    "\n",
    "![Sauron](https://i0.wp.com/middle-earth.xenite.org/files/2013/12/sauron-and-the-one-ring.jpg?fit=360%2C247&ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629bfa21-8ad9-4793-973b-13191fce26a1",
   "metadata": {},
   "source": [
    "But, there's a catch, you need to pain-stakingly craft a nice prompt to get a relevant answer. When GPT3 was first released, all the NLP tasks (summarization, QnA, translation) needed a bunch of example to be sent to a prompt. This information stuffing isn't required anymore now but you still need a few ways to pass some information.\n",
    "\n",
    "`LangChain` with it's `Prompt` construct simplifies this information stuffing helping us to truly write dynamically generated queries whose side-effect is faster iteration.\n",
    "\n",
    "Let's see a complex example, where I want to generate text on a topic based on how the popular Dragon Ball Z characters would talk. Let's write a prompt for `Vegeta, a character who is egotistical and sarcastic`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4d2e2309-24d3-461a-84ac-1d28e7149af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vegeta_prompt = \"Write 50 words on Global Warming in the tone of Vegeta, a character who is egotistical and sarcastic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "441b38c0-420a-4bd3-b6b3-ae79b5956ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n1. Global Warming? Bah, how could a puny planet like this one possibly affect the universe's climate.\\n2. Typical humans, thinking they can do anything they please and the universe will remain unchanged.\\n3. Global Warming? I suppose it is the least of this planet's problems.\\n4. I guess I should be grateful that Global Warming isn't any worse than it is.\\n5. If I wanted to destroy the planet, I'd just wait until Global Warming does it for me.\\n6. Global Warming? It's like the universe is trying to tell me something, but I'm not sure what.\\n7. Don't worry about Global Warming, I'll just use my superior Saiyan strength to fix it.\\n8. Global Warming? I don't even have time to think about it, I'm too busy trying to save the universe.\\n9. I bet I could stop Global Warming with one glance of my powerful glare.\\n10. Global Warming? What a pathetic attempt to ruin the planet. I'll take care of it.\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt35_turbo(vegeta_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f97af507-bed0-44e3-badc-942e28d7ea00",
   "metadata": {},
   "source": [
    "Very impressive! Now if I've to write it in the tone of Gohan who's a nerd and serious kid, I'd have to copy paste a lot of stuff. But with `LangChain`'s `PromptTemplate` we can do a bunch of code optimization easily.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "afb915f2-d4bc-4186-bf32-368cfb2a38b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "45a2d68f-fea8-4121-b989-799a29828b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Write 50 words on Global Warming in the tone of {character}, character who is {personality}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62446826-2eb0-49cd-8028-c6ae129c457e",
   "metadata": {},
   "source": [
    "I can list a bunch of characters in a list and just write a loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5a47d356-6b61-4e0d-8e8d-18e048a030eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = [\n",
    "    {\"character\": \"Vegeta\", \"personality\": \"egotistical and sarcastic\"},\n",
    "    {\"character\": \"Gohan\", \"personality\": \"nerdy and serious\"},\n",
    "    {\"character\": \"Chichi\", \"personality\": \"angry and strong woman\"},\n",
    "    {\"character\": \"Bulma\", \"personality\": \"scientist and opinionated\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1d92a64a-9394-45ea-8fbb-d36d1ab5302b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mPromptTemplate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minput_variables\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moutput_parser\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseOutputParser\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpartial_variables\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtemplate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtemplate_format\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'f-string'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvalidate_template\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Schema to represent a prompt for an LLM.\n",
       "\n",
       "Example:\n",
       "    .. code-block:: python\n",
       "\n",
       "        from langchain import PromptTemplate\n",
       "        prompt = PromptTemplate(input_variables=[\"foo\"], template=\"Say {foo}\")\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Create a new model by parsing and validating input data from keyword arguments.\n",
       "\n",
       "Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/personal-projects/ams-builder-workshop/.venv/lib/python3.10/site-packages/langchain/prompts/prompt.py\n",
       "\u001b[0;31mType:\u001b[0m           ModelMetaclass\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "85a1de58-dd0f-449e-9779-93d09ed1d0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(input_variables=[\"character\", \"personality\"], template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e7f47394-05a8-451d-bca1-d7191450213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d4a87133-5f49-4e35-9bf8-b09546a75470",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=gpt35_turbo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "eb468eb4-3ca6-4445-8d8a-d2b13be94a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    }
   ],
   "source": [
    "result = llm_chain.generate(characters) # Multiple prompts in one simple function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f694da7-83d0-41ef-8bc7-cc95d3671eac",
   "metadata": {},
   "source": [
    "Langchain also retries on my behalf automatically without making me write extra code! How good is that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0987655c-9d9c-485e-86b1-8870619641b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Vegeta ::: \\n\\n1. Global Warming? Pfft, more like Global Warning--for all the fools who can't see a disaster coming.\\n\\n2. All of you squabbling about who's to blame? Get your act together and fix it already!\\n\\n3. I'm not impressed by your feeble efforts. Do something real!\\n\\n4. You fools. Don't you know that your activities are destroying our planet?\\n\\n5. Stop being so naive! Technology isn't going to save us, you know.\\n\\n6. Quit talking about it and start doing something about it!\\n\\n7. I can't believe how irresponsible you all are.\\n\\n8. You can't ignore the facts any longer. Global Warming is real and it's caused by humans.\\n\\n9. You can try to blame it on something else, but it won't make the problem go away.\\n\\n10. Denial won't save you from the consequences of your actions.\\n\\n11. No matter how much you deny it, you can't ignore the effects of Global Warming.\\n\\n12. Ignoring the truth won't make it go away.\\n\\n13. Stop pretending it's not happening and do something to\",\n",
       " 'Gohan ::: \\n\\nGlobal warming is a serious issue that we must address. We need to reduce emissions of greenhouse gases and increase our use of renewable energy sources such as solar and wind power. We must also take steps to reduce deforestation, as trees are essential for carbon sequestration. We must also reduce our reliance on fossil fuels, as burning them is a major source of greenhouse gases. We must take action now, or the consequences of global warming will be devastating for future generations. We must educate ourselves and others on the causes and effects of global warming, and strive to make a difference. Together, we can make a real impact in reducing global warming.',\n",
       " 'Chichi ::: \\n\\nGlobal Warming is a serious issue that needs to be addressed. It is having a devastating effect on the planet and the environment. We need to take drastic measures to reduce emissions and stop this problem from getting worse. We must stop burning fossil fuels and switch to renewable energy sources. We need more public awareness on what we can do to reduce our carbon footprint. We cannot continue to ignore this problem and expect it to go away on its own. We must take action now to save our planet and future generations. Our future depends on it!',\n",
       " \"Bulma ::: \\n\\nGlobal Warming is the biggest threat to our planet right now. It is caused by human activity, such as burning fossil fuels, that releases carbon dioxide into the atmosphere. This contributes to a rapid increase of our planet's average temperature which is causing devastating environmental changes. We can't ignore it any longer. We must take action now to reduce our emissions and find alternative sources of energy. I believe renewable energy sources such as solar and wind power, are the most sustainable and eco-friendly options. We also need to focus on planting more trees and creating more green spaces. We need to make a conscious effort to conserve energy and reduce our carbon footprint. We must all take responsibility and ensure that our actions today don't cause irreversible damage in the future. The time to act is now.\"]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f\"{characters[idx]['character']} ::: {gen[0].text}\" for idx, gen in enumerate(result.generations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a4264a-4cfc-4269-aa61-c94d77473241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90e6861-1f89-48e1-8108-ccabdf9bd487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1384d252-ab92-433c-9984-79594319ff64",
   "metadata": {},
   "source": [
    "## Understanding Memory via ChatBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659e3f76-73d8-4617-b4fe-9375cf11fb70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ac58a00-7c7d-4afb-9b20-c5f71ee3ac78",
   "metadata": {},
   "source": [
    "## Understanding Indexes via Document QnA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5a83cc-6f63-4c78-8597-ea315f40465b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fbf0ce1-cbf0-4284-bbc2-1e56cf4c014d",
   "metadata": {},
   "source": [
    "## Introduction to Agents and Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c71b77-5cac-4731-9630-c93492845805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3cd3ac8-29cb-4433-88a5-9647f6554fcd",
   "metadata": {},
   "source": [
    "## Semantic Search via LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389413a3-3bf5-46b1-ad6a-74c760776208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7b2453b-70ca-4f58-884d-6aea1aaaeefa",
   "metadata": {},
   "source": [
    "## Query csv/excel data via LangChain (plus LlamaIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3e355e-0801-4508-ac97-5f991b709655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16d189e-af75-4887-97f5-ce67fb6f431b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
