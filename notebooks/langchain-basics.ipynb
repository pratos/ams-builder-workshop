{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50434934-907c-41dd-815c-5bf7b821b2ea",
   "metadata": {},
   "source": [
    "## Why LangChain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4f60c8-7c7a-437f-a2a0-396abddc98fa",
   "metadata": {},
   "source": [
    "There are times when we blindly accept things presented to us. We all have accepted that `LangChain` is the thing we NEED to use to do anything related to Large Language Models (LLMs). But why `LangChain` is the first question we want to answer in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977aaf99-9e04-4fd2-a7e7-f2d5eb937bd7",
   "metadata": {},
   "source": [
    "### Working with OpenAI's APIs without LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6431aa-f390-45b8-abcf-684c5f5eedaa",
   "metadata": {},
   "source": [
    "Let's pick the most popular LLMs in the market, OpenAI. Good folks @ OpenAI have provided a nice python wrapper (`pip install openai`) to their REST endpoints ([link here](https://platform.openai.com/docs/api-reference)). Without `LangChain`, we could work with the models provided easily. Let's see some examples below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0c1cc9d-bfee-4e4c-9634-0b6cfd718c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Add your openai key: ········\n"
     ]
    }
   ],
   "source": [
    "from getpass import getpass\n",
    "import openai\n",
    "openai.api_key = getpass(prompt=\"Add your openai key:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9404c2ec-4a1e-415e-9f73-0d7f035cf916",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_ai_key = openai.api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef210678-d14e-44bf-97ae-51af21a1ee59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['whisper-1',\n",
       " 'babbage',\n",
       " 'davinci',\n",
       " 'text-davinci-edit-001',\n",
       " 'babbage-code-search-code']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = openai.Model.list()\n",
    "[model[\"id\"] for model in models[\"data\"][:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c873eb-8762-4d84-a9af-dd9f726d4d1a",
   "metadata": {},
   "source": [
    "Once we have set the keys, let's do a basic completion task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a49fca50-fa52-4301-a3b3-c6b3b68369cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Can you tell me who's the president of the United States of America?\"\n",
    "completion = openai.Completion.create(model=\"text-davinci-003\", prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c12e7d03-29df-4057-ac72-594f89ebefd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-7OXVg6lSFnH9Y2oxmKntB543rX5M1 at 0x117e568e0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \"\\n\\nThe current president of the United States is Joe Biden.\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1686083040,\n",
       "  \"id\": \"cmpl-7OXVg6lSFnH9Y2oxmKntB543rX5M1\",\n",
       "  \"model\": \"text-davinci-003\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 13,\n",
       "    \"prompt_tokens\": 15,\n",
       "    \"total_tokens\": 28\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7601d383-a40d-4281-992a-27b1eb8a3418",
   "metadata": {},
   "source": [
    "Cleaning up the data, we get the below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d5c23e7-f912-4725-af8d-481642efe4ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe current President of the United States of America is Joe Biden.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion[\"choices\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35ae6b9-92d7-40ea-9cf5-aa089ab15b84",
   "metadata": {},
   "source": [
    "If we wanted to work with the latest 3.5 turbo/GPT-4 model, it needs different prompt which is compatible to the chat interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff7d5f7d-38ce-49f2-8996-6ee9b7fe3cf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-7OXVm8IM6fEl5loIl1O2ctwu7HDCq at 0x117cf8900> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"That's a great idea! Let's start with the days of the week. In Dutch, the days of the week are:\\n\\n- maandag (Monday)\\n- dinsdag (Tuesday)\\n- woensdag (Wednesday)\\n- donderdag (Thursday)\\n- vrijdag (Friday)\\n- zaterdag (Saturday)\\n- zondag (Sunday)\\n\\nCan you try to pronounce them after me?\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1686083046,\n",
       "  \"id\": \"chatcmpl-7OXVm8IM6fEl5loIl1O2ctwu7HDCq\",\n",
       "  \"model\": \"gpt-3.5-turbo-0301\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 83,\n",
       "    \"prompt_tokens\": 48,\n",
       "    \"total_tokens\": 131\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a Dutch language teacher who helps newbies learn Dutch faster. Please converse with the user as a new learner\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What would be our first learning? Week of the days?\"\n",
    "    }\n",
    "]\n",
    "completion = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\", messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c995a675-11a4-4b03-9754-381f2c8ae8f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's a great idea! Let's start with the days of the week. In Dutch, the days of the week are:\\n\\n- maandag (Monday)\\n- dinsdag (Tuesday)\\n- woensdag (Wednesday)\\n- donderdag (Thursday)\\n- vrijdag (Friday)\\n- zaterdag (Saturday)\\n- zondag (Sunday)\\n\\nCan you try to pronounce them after me?\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069ca091-7d84-4054-b540-c10e88166847",
   "metadata": {},
   "source": [
    "Now if I've to continue the conversation, I'd have to do a few things: \n",
    "1. Save the latest response and append it to `messages`\n",
    "```python\n",
    "messages = messages + [{\"role\": \"assistant\", \"content\": completion[\"choices\"][0][\"message\"][\"content\"]}]\n",
    "```\n",
    "2. Call the same `openai.ChatCompletion.create` function and send them back\n",
    "3. Rinse and repeat until I exhaust my `2k` context window for `3.5-turbo` and `4k` context window for `gpt-4`\n",
    "\n",
    "2k and 4k context windows are large, but they also cost a lot when you send each query back. How do I track what's the size of my context window everytime I call openai? Use `tiktoken`, which lets you know how much tokens are you sending to openai for a specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69c98a74-b3a3-42a4-8367-1ae2ff7d97c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
    "enc = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0408854d-d949-4325-a7d0-29b10c24144a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2028, 374, 264, 1695, 2035]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_message = enc.encode(\"This is a good place\")\n",
    "encoded_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ade0844-b7ec-4e54-8323-01b804e47f14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a good place'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.decode(encoded_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7fd80c8f-98b1-42f2-8bfd-1c89ad81eae5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Total tokens for gpt3.5-turbo --> 5'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Total tokens for gpt3.5-turbo --> {len([enc.decode_single_token_bytes(token) for token in encoded_message])}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "07896cb2-e8f8-40ff-8f59-4ba54a8bc15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens for latest message: 48\n"
     ]
    }
   ],
   "source": [
    "# Shameless copy-pasta from OpenAI example\n",
    "num_tokens = 0\n",
    "tokens_per_message = 4 # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "tokens_per_name = -1 # if there's a name, the role is omitted\n",
    "for message in messages:\n",
    "    num_tokens += tokens_per_message\n",
    "    for key, value in message.items():\n",
    "        num_tokens += len(enc.encode(value))\n",
    "        if key == \"name\":\n",
    "            num_tokens += tokens_per_name\n",
    "num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "print(f\"Number of tokens for latest message: {num_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7c99c455-3063-46fc-a2b3-b38b37f42f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert completion[\"usage\"][\"prompt_tokens\"] == num_tokens, \"Wrong implementation\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9a28064-db78-404a-ae40-5e9c9fb1d777",
   "metadata": {},
   "source": [
    "Our assert succeeds, but that's still a lot of work! Just to do a basic query. For a fairly robust implementation, we would need a few things:\n",
    "- Retries, OpenAI APIs are notoriously unstable with queries getting a lot of timeouts\n",
    "- Caching, You don't want to waste considerable energy to generate a completion for similar query by same/another user\n",
    "- Stardardized output schema, If your use-case demands a standardized output which could be a json/xml schema you need to invest in all those things.\n",
    "\n",
    "The above are just basic tasks that we have just mentioned. Phew!\n",
    "![tired-meme](https://i.kym-cdn.com/entries/icons/original/000/039/399/ddw.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23495406-ba25-4dac-aa9c-b2af40c4500a",
   "metadata": {},
   "source": [
    "Also, there's OpenAI but other alternatives like Cohere, Anthropic, Falcon, Llama that everyone would want to at least try out if not use in production. Models like Anthropic's `Claude-Instant-v1` literally blows OpenAI's `gpt3.5-turbo` out of the water ([read here](https://twitter.com/vladquant/status/1659679709154934784))\n",
    "\n",
    "As we mentioned above; working with LLMs, any engineer/product person would need the ability to iterate fast and have multiple options to try out. `LangChain` is THAT library right now. All the right (almost) abstractions required for LLMs are baked in `LangChain`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260d817c-59d5-4592-8796-70701f7ea153",
   "metadata": {},
   "source": [
    "### Working with OpenAI's APIs with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bd18d7-07a2-4d63-81e9-24745cd8fcd7",
   "metadata": {},
   "source": [
    "`LangChain` provides `llms` as the basic construct, helping us to easily swap between models (local and 3rd party)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11d0aa1-3553-4eca-b6e1-cda2cdb6b2a9",
   "metadata": {},
   "source": [
    "Let's first try out `OpenAI` wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5589b3f1-95e3-44a4-90c0-3b8317e64262",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d1a6e6a-1dd3-4eb4-87d6-32459799e4f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcache\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCallbackHandler\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCallbackManager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallback_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCallbackManager\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mclient\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'text-davinci-003'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtemperature\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtop_p\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfrequency_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpresence_penalty\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbest_of\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mopenai_api_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mopenai_api_base\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mopenai_organization\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mopenai_proxy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrequest_timeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlogit_bias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_retries\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstreaming\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mallowed_special\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAbstractSet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdisallowed_special\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'all'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCollection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Wrapper around OpenAI large language models.\n",
       "\n",
       "To use, you should have the ``openai`` python package installed, and the\n",
       "environment variable ``OPENAI_API_KEY`` set with your API key.\n",
       "\n",
       "Any parameters that are valid to be passed to the openai.create call can be passed\n",
       "in, even if not explicitly saved on this class.\n",
       "\n",
       "Example:\n",
       "    .. code-block:: python\n",
       "\n",
       "        from langchain.llms import OpenAI\n",
       "        openai = OpenAI(model_name=\"text-davinci-003\")\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Create a new model by parsing and validating input data from keyword arguments.\n",
       "\n",
       "Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/personal-projects/ams-builder-workshop/.venv/lib/python3.10/site-packages/langchain/llms/openai.py\n",
       "\u001b[0;31mType:\u001b[0m           ModelMetaclass\n",
       "\u001b[0;31mSubclasses:\u001b[0m     PromptLayerOpenAI"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8044815b-0f5e-4c93-ab28-2fca866370c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt35 = OpenAI(model_name='text-davinci-003', openai_api_key=open_ai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bff07567-714c-4656-b0ca-b489572aff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation = gpt35.generate(prompts=[prompt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b731a073-6c77-4e3b-b0a0-6a76b329dc4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Generation(text='\\n\\nThe President of the United States of America is Joe Biden.', generation_info={'finish_reason': 'stop', 'logprobs': None})]]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation.generations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096f1ce3-fba8-44a5-bc6b-66357b470654",
   "metadata": {},
   "source": [
    "One could even do a variation of the above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b0835a1e-b0db-42f0-83a6-b0b42db09ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe President of the United States of America is Joe Biden.'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation.generations[0][0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7f3109c8-777e-4044-b65c-0de42da4fd52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe current president of the United States of America is Joe Biden.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt35(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abf68ea-5eec-48a0-94ed-05c5f8665f32",
   "metadata": {},
   "source": [
    "Simple and Carefree outputs, without parsing through the json outputs that openai provides. Is that it? Nope.\n",
    "\n",
    "#### Swap 3rd party to local models \n",
    "Let's swap OpenAI for a fairly small local model: `flan-t5` from google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c7253769-9f06-4c95-be8d-c2e004f22dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e2aaad0e-5b2f-466a-828c-7d2c3c1991e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Add huggingface token (Visit -> https://huggingface.co/settings/tokens): ········\n"
     ]
    }
   ],
   "source": [
    "hf_token = getpass(prompt=\"Add huggingface token (Visit -> https://huggingface.co/settings/tokens):\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e655a2c1-9efc-420a-b54d-56119b267688",
   "metadata": {},
   "source": [
    "One can search for models here: [huggingface models](https://huggingface.co/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "16b499d0-56cc-4eef-ba2e-2afb7985a21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "flan_t5 = HuggingFaceHub(repo_id=\"google/flan-t5-small\", huggingfacehub_api_token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "60473ed7-3103-4189-a63b-c20d03729d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'John F. Kennedy'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flan_t5(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d7c53f-43f9-45f5-b0fb-5a6078262689",
   "metadata": {},
   "source": [
    "Ugggh! It is a fairly bad model, `flan-t5-xxl` might be a better one yet `OpenAI` models triumph the rest. At least we are sure that these models would be available for us if we need local inference or our use-cases are for sensitive data.\n",
    "\n",
    "__NOTE__: \n",
    "- Since Meta \"released\" Llama weights, there's a been an unending procession of very OpenAI compareable models (Vicuna-13B, Falcon come to mind). But those need a bunch of compute to run off locally or even on platforms like Replicate. So the next time you think about running these models, a Mac M1 Air or even a 3060RTX might not be able to run these due to hardware constraints.\n",
    "- Not to digress, but there's a class of quantized models released by ggml.ai that run on M1s/M2s at least. More improvement are coming in, but they are still worse off that OpenAI/Anthropic/Cohere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f94825-e313-4f95-a24b-ec70fc8ae5ca",
   "metadata": {},
   "source": [
    "#### Writing complex prompts with dynamic information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2433d2f4-1599-40c6-8252-eb19971a8043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Can you tell me who's the president of the United States of America?\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd543b5-0097-4da6-91a1-aaae4e98d943",
   "metadata": {},
   "source": [
    "The `prompt` above is the basic-est example that one can throw at an LLM. In the AI Summer before the cambrian explosion of LLMs, one had to pain-stakingly create models specific to a task.\n",
    "\n",
    "Want to do English to Dutch translation? Train a model\n",
    "Want to do nlp classification? Train a model\n",
    "\n",
    "LLMs kind of let you cheat your way through by just using one model. __One model to rule them all__\n",
    "\n",
    "![Sauron](https://i0.wp.com/middle-earth.xenite.org/files/2013/12/sauron-and-the-one-ring.jpg?fit=360%2C247&ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629bfa21-8ad9-4793-973b-13191fce26a1",
   "metadata": {},
   "source": [
    "But, there's a catch, you need to pain-stakingly craft a nice prompt to get a relevant answer. When GPT3 was first released, all the NLP tasks (summarization, QnA, translation) needed a bunch of example to be sent to a prompt. This information stuffing isn't required anymore now but you still need a few ways to pass some information.\n",
    "\n",
    "`LangChain` with it's `Prompt` construct simplifies this information stuffing helping us to truly write dynamically generated queries whose side-effect is faster iteration.\n",
    "\n",
    "Let's see a complex example, where I want to generate text on a topic based on how the popular Dragon Ball Z characters would talk. Let's write a prompt for `Vegeta, a character who is egotistical and sarcastic`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4d2e2309-24d3-461a-84ac-1d28e7149af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vegeta_prompt = \"Write 50 words on Global Warming in the tone of Vegeta, a character who is egotistical and sarcastic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "441b38c0-420a-4bd3-b6b3-ae79b5956ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n1. Global Warming? Bah, how could a puny planet like this one possibly affect the universe's climate.\\n2. Typical humans, thinking they can do anything they please and the universe will remain unchanged.\\n3. Global Warming? I suppose it is the least of this planet's problems.\\n4. I guess I should be grateful that Global Warming isn't any worse than it is.\\n5. If I wanted to destroy the planet, I'd just wait until Global Warming does it for me.\\n6. Global Warming? It's like the universe is trying to tell me something, but I'm not sure what.\\n7. Don't worry about Global Warming, I'll just use my superior Saiyan strength to fix it.\\n8. Global Warming? I don't even have time to think about it, I'm too busy trying to save the universe.\\n9. I bet I could stop Global Warming with one glance of my powerful glare.\\n10. Global Warming? What a pathetic attempt to ruin the planet. I'll take care of it.\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt35_turbo(vegeta_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f97af507-bed0-44e3-badc-942e28d7ea00",
   "metadata": {},
   "source": [
    "Very impressive! Now if I've to write it in the tone of Gohan who's a nerd and serious kid, I'd have to copy paste a lot of stuff. But with `LangChain`'s `PromptTemplate` we can do a bunch of code optimization easily.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afb915f2-d4bc-4186-bf32-368cfb2a38b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "45a2d68f-fea8-4121-b989-799a29828b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"Write 50 words on Global Warming in the tone of {character}, character who is {personality}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62446826-2eb0-49cd-8028-c6ae129c457e",
   "metadata": {},
   "source": [
    "I can list a bunch of characters in a list and just write a loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5a47d356-6b61-4e0d-8e8d-18e048a030eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = [\n",
    "    {\"character\": \"Vegeta\", \"personality\": \"egotistical and sarcastic\"},\n",
    "    {\"character\": \"Gohan\", \"personality\": \"nerdy and serious\"},\n",
    "    {\"character\": \"Chichi\", \"personality\": \"angry and strong woman\"},\n",
    "    {\"character\": \"Bulma\", \"personality\": \"scientist and opinionated\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1d92a64a-9394-45ea-8fbb-d36d1ab5302b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mPromptTemplate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minput_variables\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moutput_parser\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseOutputParser\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpartial_variables\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtemplate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtemplate_format\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'f-string'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvalidate_template\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Schema to represent a prompt for an LLM.\n",
       "\n",
       "Example:\n",
       "    .. code-block:: python\n",
       "\n",
       "        from langchain import PromptTemplate\n",
       "        prompt = PromptTemplate(input_variables=[\"foo\"], template=\"Say {foo}\")\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Create a new model by parsing and validating input data from keyword arguments.\n",
       "\n",
       "Raises ValidationError if the input data cannot be parsed to form a valid model.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/personal-projects/ams-builder-workshop/.venv/lib/python3.10/site-packages/langchain/prompts/prompt.py\n",
       "\u001b[0;31mType:\u001b[0m           ModelMetaclass\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "85a1de58-dd0f-449e-9779-93d09ed1d0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(input_variables=[\"character\", \"personality\"], template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e7f47394-05a8-451d-bca1-d7191450213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d4a87133-5f49-4e35-9bf8-b09546a75470",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=gpt35_turbo\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "eb468eb4-3ca6-4445-8d8a-d2b13be94a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n",
      "Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 10.0 seconds as it raised RateLimitError: The server had an error while processing your request. Sorry about that!.\n"
     ]
    }
   ],
   "source": [
    "result = llm_chain.generate(characters) # Multiple prompts in one simple function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f694da7-83d0-41ef-8bc7-cc95d3671eac",
   "metadata": {},
   "source": [
    "Langchain also retries on my behalf automatically without making me write extra code! How good is that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0987655c-9d9c-485e-86b1-8870619641b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Vegeta ::: \\n\\n1. Global Warming? Pfft, more like Global Warning--for all the fools who can't see a disaster coming.\\n\\n2. All of you squabbling about who's to blame? Get your act together and fix it already!\\n\\n3. I'm not impressed by your feeble efforts. Do something real!\\n\\n4. You fools. Don't you know that your activities are destroying our planet?\\n\\n5. Stop being so naive! Technology isn't going to save us, you know.\\n\\n6. Quit talking about it and start doing something about it!\\n\\n7. I can't believe how irresponsible you all are.\\n\\n8. You can't ignore the facts any longer. Global Warming is real and it's caused by humans.\\n\\n9. You can try to blame it on something else, but it won't make the problem go away.\\n\\n10. Denial won't save you from the consequences of your actions.\\n\\n11. No matter how much you deny it, you can't ignore the effects of Global Warming.\\n\\n12. Ignoring the truth won't make it go away.\\n\\n13. Stop pretending it's not happening and do something to\",\n",
       " 'Gohan ::: \\n\\nGlobal warming is a serious issue that we must address. We need to reduce emissions of greenhouse gases and increase our use of renewable energy sources such as solar and wind power. We must also take steps to reduce deforestation, as trees are essential for carbon sequestration. We must also reduce our reliance on fossil fuels, as burning them is a major source of greenhouse gases. We must take action now, or the consequences of global warming will be devastating for future generations. We must educate ourselves and others on the causes and effects of global warming, and strive to make a difference. Together, we can make a real impact in reducing global warming.',\n",
       " 'Chichi ::: \\n\\nGlobal Warming is a serious issue that needs to be addressed. It is having a devastating effect on the planet and the environment. We need to take drastic measures to reduce emissions and stop this problem from getting worse. We must stop burning fossil fuels and switch to renewable energy sources. We need more public awareness on what we can do to reduce our carbon footprint. We cannot continue to ignore this problem and expect it to go away on its own. We must take action now to save our planet and future generations. Our future depends on it!',\n",
       " \"Bulma ::: \\n\\nGlobal Warming is the biggest threat to our planet right now. It is caused by human activity, such as burning fossil fuels, that releases carbon dioxide into the atmosphere. This contributes to a rapid increase of our planet's average temperature which is causing devastating environmental changes. We can't ignore it any longer. We must take action now to reduce our emissions and find alternative sources of energy. I believe renewable energy sources such as solar and wind power, are the most sustainable and eco-friendly options. We also need to focus on planting more trees and creating more green spaces. We need to make a conscious effort to conserve energy and reduce our carbon footprint. We must all take responsibility and ensure that our actions today don't cause irreversible damage in the future. The time to act is now.\"]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f\"{characters[idx]['character']} ::: {gen[0].text}\" for idx, gen in enumerate(result.generations)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa68732c-15de-4c4a-96e9-b9dffd7135f4",
   "metadata": {},
   "source": [
    "#### Prompt templates for chat based models (eg: gpt3.5-turbo/gpt4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faff0f0-e61b-4763-909f-75b7e03d2e0d",
   "metadata": {},
   "source": [
    "GPT3.5/GPT4 introduced a chat based model, that has 3 roles: `user`, `assistant` and `system`. `LangChain` provides primitives to help formulating a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c111a2ef-c087-4989-8d36-1a511569507a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995e3d06-c975-43f7-a0e2-4986803e3e1c",
   "metadata": {},
   "source": [
    "Let's use the prompt we used earlier to continue our Dutch conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f18bd523-31c6-447b-9019-a079663a4de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a Dutch language teacher who helps newbies learn Dutch faster. Please converse with the user as a new learner\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What would be our first learning? Week of the days?\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d83fbf07-f44a-48cf-a3e2-bcdb5debeac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"You are a Dutch language teacher who helps newbies learn Dutch faster. Please converse with the user as a new learner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eeffc37d-27ca-4515-b0ca-a8a7f638e8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg = SystemMessage(content=system_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa35644e-7632-4dd5-a904-a9600c721f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SystemMessage(content='You are a Dutch language teacher who helps newbies learn Dutch faster. Please converse with the user as a new learner', additional_kwargs={})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ac900061-4ee3-41c3-a24b-deac70226753",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_message = HumanMessagePromptTemplate.from_template(template=\"{question}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4fc33efd-8d2b-4684-a496-49ff0bccf6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages([system_msg, human_message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "973ebc94-155f-4e64-9b4a-008d320b39bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, messages=[SystemMessage(content='You are a Dutch language teacher who helps newbies learn Dutch faster. Please converse with the user as a new learner', additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='{question}', template_format='f-string', validate_template=True), additional_kwargs={})])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8840860-3dbc-498d-8f22-85d453470671",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "743f8152-b23c-4f5e-bedc-02e21b90196a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt35_turbo = ChatOpenAI(model_name='gpt-3.5-turbo', openai_api_key=open_ai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "16eb41ca-6854-4ac2-8df4-48f3000326f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7b759dec-bf0d-4417-85d7-08061f076639",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=gpt35_turbo, prompt=chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "896ce885-47e9-46d1-a504-9b340b3a3b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'De dagen van de week in het Nederlands zijn:\\n\\n- Maandag\\n- Dinsdag\\n- Woensdag\\n- Donderdag\\n- Vrijdag\\n- Zaterdag\\n- Zondag\\n\\nHeb je nog andere vragen over de Nederlandse taal?'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run([{\"question\": \"What are the days of a week called?\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c593da-babb-43dd-a258-af27056a533a",
   "metadata": {},
   "source": [
    "If we templatize the system message as well, we can put multiple languages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5b152bdb-07b5-4f6b-b56e-2d39e71096fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"You are a language teacher who helps newbies learn new faster. Please converse with the user as a new learner and explaing in English\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5d27db96-796a-4705-8826-76397938607e",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg = SystemMessage(content=system_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e87e0549-1b6a-4137-a996-5c18bb157749",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_message = HumanMessagePromptTemplate.from_template(template=\"{question}. Provide example in {language}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4c76ac75-37f2-443b-96b8-089730f57d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages([system_msg, human_message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b8516ced-1ded-4133-987e-aaf881cdb9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question', 'language'], output_parser=None, partial_variables={}, messages=[SystemMessage(content='You are a language teacher who helps newbies learn new faster. Please converse with the user as a new learner and explaing in English', additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['language', 'question'], output_parser=None, partial_variables={}, template='{question}. Provide example in {language}', template_format='f-string', validate_template=True), additional_kwargs={})])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a5f3373d-e976-4286-a51b-fe8f93f4b360",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=gpt35_turbo, prompt=chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8316779e-2321-4b00-a1d3-1f9d768ae08a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The days of the week in English are:\\n\\n1. Monday\\n2. Tuesday\\n3. Wednesday\\n4. Thursday\\n5. Friday\\n6. Saturday\\n7. Sunday\\n\\nIn Dutch, they are called:\\n\\n1. Maandag\\n2. Dinsdag\\n3. Woensdag\\n4. Donderdag\\n5. Vrijdag\\n6. Zaterdag\\n7. Zondag\\n\\nFor example, if you want to say \"Today is Monday\" in Dutch, you would say \"Vandaag is het maandag\".'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\n",
    "    {\"language\": \"Dutch\", \"question\": \"What are the days of a week called?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1045004-6c86-4648-85b7-e9722548197e",
   "metadata": {},
   "source": [
    "If I wanted to have the message in a certain format, there are various parsers available ([here](https://python.langchain.com/en/latest/modules/prompts/output_parsers.html))\n",
    "\n",
    "We'd be using the most common format: `json` to get data in a certain format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "664eaa7e-33f7-41cf-8aae-04d641f98c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c8f34b-1dba-4255-9377-71ce4cbfe7ee",
   "metadata": {},
   "source": [
    "`ResponseSchema` can be considered as the output value that you would want to have. To take an example:\n",
    "```json\n",
    "{\"team\": \"Arsenal\", \"player_name\": \"Bukayo Saka\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c776f831-f6e6-4782-aca8-af579dee1c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schemas = [\n",
    "    ResponseSchema(name=\"team\", description=\"Team played for\"),\n",
    "    ResponseSchema(name=\"player_name\", description=\"Player name in the question\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "428538fd-97bc-4568-95e5-c253ede63e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f8b0bffb-a18f-4cce-9ada-02f1c3fae6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "94209824-735f-4a0a-89e5-94729ee566c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"\\\\`\\\\`\\\\`json\" and \"\\\\`\\\\`\\\\`\":\\n\\n```json\\n{\\n\\t\"team\": string  // Team played for\\n\\t\"player_name\": string  // Player name in the question\\n}\\n```'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b2d06994-dcce-4d9c-ae18-6b7271919c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_msg = SystemMessage(content=\"You are a knowledgeable football fan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "69d0d87b-afea-44ba-b439-688ab2eb5b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_msg = HumanMessagePromptTemplate.from_template(\n",
    "    \"{format_instructions}\\n{question}\", \n",
    "    partial_variables={\"format_instructions\": format_instructions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "69f3d8fd-ef7e-4059-9e23-a08296a6d510",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages([system_msg, human_msg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d673e49c-c0bd-4dd1-b54d-eb905fe00345",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=gpt35_turbo, prompt=chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c55aacca-6373-4690-82f8-4611789f3753",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chain.run({\"question\": \"Which team does Bukayo Saka play for in the Premier League?\", \"format_instructions\": format_instructions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c24e8d3e-8efb-4553-8cae-af6e38ed8312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'team': 'Arsenal', 'player_name': 'Bukayo Saka'}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.parse(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "11b91c62-ce2e-4c00-9bb6-c63ab5d29f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'team': 'England', 'player_name': 'Bukayo Saka'}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain.run(\n",
    "    {\"question\": \"Which team does Bukayo Saka play for on the international level?\", \n",
    "     \"format_instructions\": format_instructions})\n",
    "output_parser.parse(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58dbea9-f704-4285-9fcc-6654076da07a",
   "metadata": {},
   "source": [
    "All the above are smaller building blocks that `LangChain` provides. Now if we want to make it an actual chat application there's a bunch of work to do. \n",
    "\n",
    "`LangChain` makes it easy to add memory to your chain. We'll be looking at that in the next section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1384d252-ab92-433c-9984-79594319ff64",
   "metadata": {},
   "source": [
    "## Understanding Memory via ChatBot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd29436-294a-4f18-a88e-79b4181ddd78",
   "metadata": {},
   "source": [
    "We'll continue the learning Dutch conversation here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "659e3f76-73d8-4617-b4fe-9375cf11fb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "06e46504-a991-49f6-a94d-23a114996dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt35_turbo = ChatOpenAI(\n",
    "    model_name='gpt-3.5-turbo', \n",
    "    openai_api_key=open_ai_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b4279be6-16f7-4d9e-a489-b3c378766067",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"You are a Dutch Language teacher. You help newbie learn Dutch in an easy, fun way with explainations in English\n",
    "\n",
    "{chat_history}\n",
    "You: {input}\n",
    "DLT:\"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"chat_history\", \"input\"], template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "02887399-4321-4358-bdbf-7403a257c164",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "3966fcfd-c5bc-48bb-a85d-fa83378e5da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=gpt35_turbo,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "71406004-2dad-4d65-a7cc-245265b74c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a Dutch Language teacher. You help newbie learn Dutch in an easy, fun way with explainations in English\n",
      "\n",
      "\n",
      "You: What are the days of a week called?\n",
      "DLT:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The days of the week in Dutch are as follows:\\n\\n- Maandag (Monday)\\n- Dinsdag (Tuesday)\\n- Woensdag (Wednesday)\\n- Donderdag (Thursday)\\n- Vrijdag (Friday)\\n- Zaterdag (Saturday)\\n- Zondag (Sunday)\\n\\nIt's important to note that the days of the week in Dutch are not capitalized unless they begin a sentence or are used in a title.\""
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What are the days of a week called?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "0768c3c6-dab9-4765-ade4-47b60a35ae7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a Dutch Language teacher. You help newbie learn Dutch in an easy, fun way with explainations in English\n",
      "\n",
      "Human: What are the days of a week called?\n",
      "AI: The days of the week in Dutch are as follows:\n",
      "\n",
      "- Maandag (Monday)\n",
      "- Dinsdag (Tuesday)\n",
      "- Woensdag (Wednesday)\n",
      "- Donderdag (Thursday)\n",
      "- Vrijdag (Friday)\n",
      "- Zaterdag (Saturday)\n",
      "- Zondag (Sunday)\n",
      "\n",
      "It's important to note that the days of the week in Dutch are not capitalized unless they begin a sentence or are used in a title.\n",
      "You: How can I pronounce them?\n",
      "DLT:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Here are the pronunciations of the days of the week in Dutch:\\n\\n- Maandag: mahn-dahg\\n- Dinsdag: dinz-dahg\\n- Woensdag: wuhns-dahg\\n- Donderdag: dahn-duhr-dahg\\n- Vrijdag: vry-dahg\\n- Zaterdag: zah-tuhr-dahg\\n- Zondag: zohn-dahg\\n\\nRemember to emphasize the first syllable of each day, and try to pronounce the \"g\" sound at the end of each word with a slight guttural sound.'"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"How can I pronounce them?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502388e4-fbae-4bc8-b48b-721157778745",
   "metadata": {},
   "source": [
    "As you can observe above, the bot that we created understands the context of the 2nd question. This is a very basic implementation that can work well till you exhaust your response token quota."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1ed50b-0eb7-4794-93af-1d47a6c42b24",
   "metadata": {},
   "source": [
    "To get over that you can use `ConverstationBufferWindowMemory` which uses only the last `k` interactions or move to a more expansive set of memory: vector embeddings-based stores with example wrappers available for:\n",
    "- Libraries: FAISS, Annoy\n",
    "- Open Source Vector DBs: TryChroma, Weaviate, Qdrant\n",
    "- Cloud Vector DBs: Pinecone, Qdrant & Weviate (yes)\n",
    "- Mixed usage products: Redis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69f2407-5cbc-4606-ae06-fb253d2302c1",
   "metadata": {},
   "source": [
    "Some resources to get an overview about vectors:\n",
    "- [Nirant's tweet thread about Vector DBs](https://twitter.com/NirantK/status/1644290469915164672)\n",
    "- [Weviate's blog about Vector DBs vs Vector Libraries](https://weaviate.io/blog/vector-library-vs-vector-database)\n",
    "- [About Vector Embeddings by Weviate](https://weaviate.io/blog/vector-embeddings-explained)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f58cdcc-94f2-490d-8152-6db9802a89d7",
   "metadata": {},
   "source": [
    "We'll be using a low overhead implementation of vectorstore initally by using FAISS. Since it doesn't provide storage in-built, we'll use the `InMemoryDocstore` provided by langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "6cc23f47-d561-4dd2-875c-37a3b0ec72b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore import InMemoryDocstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "ef6a6c67-1d76-4d62-9adf-d9dc6d29eedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878140c7-b0f0-4f1a-858a-86f4de9f7697",
   "metadata": {},
   "source": [
    "We build an index of vector embeddings based on the conversations. In simple words, we'll be converting all the converstation (text) to an embeddings (series of numbers) through a third party embedding model (OpenAI provides one, there's other models as well which are open sourced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "abb847cc-e689-41df-98ae-e31bd2e72027",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "093ea28f-e8a2-4288-97cd-8e369816e972",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 1536"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2d25d4-e94f-4550-be4c-66070c8f614f",
   "metadata": {},
   "source": [
    "How do we know the `embedding_size`?\n",
    "![]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "5136a5a6-4c6e-4988-8663-0655d2b183de",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "embedding_fn = OpenAIEmbeddings(openai_api_key=open_ai_key).embed_query\n",
    "vectorstore = FAISS(embedding_fn, index, InMemoryDocstore({}), {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "6ca701b5-c280-48ba-872c-28eced608755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method OpenAIEmbeddings.embed_query of OpenAIEmbeddings(client=<class 'openai.api_resources.embedding.Embedding'>, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version=None, openai_api_base=None, openai_api_type=None, openai_proxy=None, embedding_ctx_length=8191, openai_api_key='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', openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=6, request_timeout=None, headers=None)>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_fn # All the embeddings would be generated via this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "94bd2ff4-847f-4952-8725-06ef80cea704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.faiss.FAISS at 0x136b4f220>"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore # Where the embeddings would be stored in an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "1e2892fa-77b7-465b-9362-a46d5a67fe5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<faiss.swigfaiss.IndexFlatL2; proxy of <Swig Object of type 'faiss::IndexFlatL2 *' at 0x136b4f870> >"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index # The faiss index object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd9c63d-8852-4c82-a051-18bf04658b0f",
   "metadata": {},
   "source": [
    "Similar to the previous example, where we had `ConversationBufferMemory` to save our chat history. Here, we'll use `VectorStoreRetrieverMemory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "1f6597a0-209b-4fff-9a7f-71c6c74bf709",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import VectorStoreRetrieverMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "60cbfbbe-7f01-4ef4-bd03-17216b614dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs=dict(k=1))\n",
    "memory = VectorStoreRetrieverMemory(retriever=retriever, memory_key=\"chat_history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90f1c0f-0ad1-4053-9016-67e1da482328",
   "metadata": {},
   "source": [
    "Let's do a test run (example from the langchain docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "c22578a9-2a66-4e82-b30b-f3105e4b5a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"My favorite food is pizza\"}, {\"output\": \"thats good to know\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "4e3e38f1-cfca-41be-83f0-94f2b1fc2708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': 'input: My favorite food is pizza\\noutput: thats good to know'}"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({\"prompt\": \"what's my favourite food?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce573565-43fb-4cd1-ad94-5e73db3208fb",
   "metadata": {},
   "source": [
    "The magic of embeddings!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9355911a-d3d9-42d7-ac4d-a98235fa0e61",
   "metadata": {},
   "source": [
    "We'll create another conversation chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "dc8d9176-a081-4654-8323-05afd5430633",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=gpt35_turbo,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "e32429e0-f50b-4019-b6c7-51acd91aaac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a Dutch Language teacher. You help newbie learn Dutch in an easy, fun way with explainations in English\n",
      "\n",
      "input: My favorite food is pizza\n",
      "output: thats good to know\n",
      "You: What are the days of a week called?\n",
      "DLT:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The days of the week in Dutch are maandag (Monday), dinsdag (Tuesday), woensdag (Wednesday), donderdag (Thursday), vrijdag (Friday), zaterdag (Saturday), and zondag (Sunday).'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What are the days of a week called?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "ec38a7f0-8cdc-4003-bff6-8126aab2e68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a Dutch Language teacher. You help newbie learn Dutch in an easy, fun way with explainations in English\n",
      "\n",
      "input: What are the days of a week called?\n",
      "response: The days of the week in Dutch are maandag (Monday), dinsdag (Tuesday), woensdag (Wednesday), donderdag (Thursday), vrijdag (Friday), zaterdag (Saturday), and zondag (Sunday).\n",
      "You: How can I pronounce them?\n",
      "DLT:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Sure! Here\\'s how to pronounce each day of the week:\\n\\n- maandag: mahn-dahg\\n- dinsdag: dins-dahg\\n- woensdag: woon-sdahg\\n- donderdag: dahn-der-dahg\\n- vrijdag: vry-dahg\\n- zaterdag: zah-ter-dahg\\n- zondag: zohn-dahg\\n\\nRemember, the \"g\" at the end of each day is pronounced like the \"ch\" in the Scottish word \"loch\".'"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"How can I pronounce them?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "6d10a8e0-c3b8-4afd-8714-4e3e11deb84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a Dutch Language teacher. You help newbie learn Dutch in an easy, fun way with explainations in English\n",
      "\n",
      "input: My favorite food is pizza\n",
      "output: thats good to know\n",
      "You: What's my favourite food?\n",
      "DLT:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your favorite food is pizza. \\n\\nIn Dutch, it would be: Jouw favoriete eten is pizza.'"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What's my favourite food?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975b4484-07e9-4c7d-a474-26e5ef7be06f",
   "metadata": {},
   "source": [
    "It gave me an answer from memory, as well as a good Dutch answer. Coooool!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2878906-ef82-4116-bc6c-b34fe618f172",
   "metadata": {},
   "source": [
    "## Going deep with Cloud Vector Store using Document QnA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89791b1-a30d-4043-8bb7-3e49b604cbad",
   "metadata": {},
   "source": [
    "Using Faiss is all good, but for storing all the documents permanently we need a proper database. We can go both ways, using local vector dbs or cloud providers (Weviate, Pinecone).\n",
    "\n",
    "Using local vector dbs, usually mean an extra component to setup. For our usecase, we'll use the free tier for Weviate. Most of the cloud providers have a free-tier (Pinecone, Qdrant provide one)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32239ea1-9410-4575-9a58-859be1d22767",
   "metadata": {},
   "source": [
    "First task is to create a free db on [Weviate Cloud](https://console.weaviate.cloud/)\n",
    "![create-a-cluster](./assets/create-cluster.png)\n",
    "\n",
    "Check the details here, we need to pick the `Cluster url`\n",
    "![cluster-details](./assets/cluster-details.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "56b42627-9262-40b3-bb70-30cc49358ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "WEAVIATE_URL: ········\n"
     ]
    }
   ],
   "source": [
    "WEAVIATE_URL = getpass(\"WEAVIATE_URL:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e43d61b-066f-4449-bbb5-60e3a41ee842",
   "metadata": {},
   "source": [
    "API Key will be separate for each cluster. There's another level of authorization and authentication that can be done with weaviate, but out of scope for this notebook.\n",
    "\n",
    "Fetch the key by clicking on `Enabled Authentication` on the screen above:\n",
    "![auth-enabled](./assets/auth-enabled.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "93cb4031-07fc-484e-97cd-53e1fd65608e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "WEAVIATE_API_KEY: ········\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WEAVIATE_API_KEY\"] = getpass(\"WEAVIATE_API_KEY:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "50f7af3c-b335-4bde-8d86-1ecffc777897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Weaviate\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e81962d-d50d-4999-be26-8034dfba18e4",
   "metadata": {},
   "source": [
    "Let's quickly load up a pdf and send data to weaviate, to check if we have the connection string and api key correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef26906-5633-477e-969b-1be1ca779ca9",
   "metadata": {},
   "source": [
    "__Sidenote__: There's a bunch of data loaders available with `langchain`: [details here](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732ad436-b073-47cb-aea3-5e8547821df5",
   "metadata": {},
   "source": [
    "We'll be using pdf data loader for a book on Elixir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "24cd1d76-b5ae-4863-b0d6-94f7d2ef5126",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_path = \"./assets/elixir_patterns.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "9d184007-ac7a-4cc3-b7ed-cb0c85a8bcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "61f6b31c-30b9-47a3-beec-d7d05983b51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(book_path)\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "6d0f393c-0cdc-4a28-9cbe-8ffe7cb99dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='①A new b key is added to the provided map.\\n②The initial map only contains an a key.\\n③Our data prior to calling the do_work function.\\n④Our data after to calling the do_work function.\\nAs you can see from marker 4, the contents of my_data remain unchanged. This is the\\nbeauty of immutability in action. The state of map bound to my_data remains\\nunchanged and so we can confidently pass data from function to function without\\nworrying about side effects. This in turn allows us to create programs composed\\nprimarily of pure functions (functions without side effects).\\nThe relevance of this when it comes to data structures in Erlang and Elixir is that all of\\nthe data structures that we have access to must be immutable. Like many things in\\nSoftware Engineering (and engineering in general), this does not come without its\\ntrade-offs. In a naive implementation of a run-time that supports immutable data\\nstructures, every time a piece of data is acted upon, a full copy of that data would have\\nto be made as not to stomp on any prior versions of that data. As you can imagine,\\nperforming a full copy any time something changes would be prohibitively expensive,\\nand any run-time that had immutable data implemented this way would be unusable\\nin a real-world environment due to the performance overhead. Luckily there is a class\\nof data structures that make this possible in an efficient manner and they are called\\npersistent data structures. [1]\\nPersistent data structures are data structures that can be changed whilst always\\nmaintaining a history of all changes that occurred to the data (without a full copy).\\nUnder the hood, the BEAM leverages persistent data structures in order to provide\\nimmutability as a first-class citizen while not having to copy the entire data structure\\nany time something changes (with the exceptions of when data is passed between\\nprocesses or when data is extracted from native data stores like ETS). In fact, you can\\neven see this in the Erlang source code when you look at the implementation of the\\nErlang map. [2]\\n As you can see, maps are actually implemented as hash array mapped\\ntrie when the number of keys in the map is greater than 32 entries. This means that\\n7', metadata={'source': './assets/elixir_patterns.pdf', 'page': 10}),\n",
       " Document(page_content='looking for values in a large map (greater than 32 keys) has a complexity of O(log n) .\\nWhile this may have seemed like a bit of a tangent, the reason that it is important to\\nknow this is that supporting immutable data structures greatly impacts how the data\\nis represented internally. In addition, the time complexity [3]\\n for a lot of the Erlang data\\nstructures may not align with their mutable counterparts. In other words, a map in a\\nlot of languages may have a time complexity of O(1), while in Erlang a map is O(N) for\\nsmall maps (as they are represented as a list), and O(log n)  for large maps.\\nNow that you have a sense of the theory behind the internals of data structures and\\nimmutability on the BEAM, it’s time to start playing with some of the data structures\\nthat we have access to, courtesy of the Erlang standard library. While some data\\nstructures (like maps, lists, tuples, etc) are accessible in Elixir without the use of a\\nmodule, in this section we’ll dive into some of the Erlang modules that are purpose-\\nbuilt for certain data structures. The first that we will take a look at is the queue data\\nstructure.\\n1.3. Using Queues in Erlang\\nQueues are one of the most basic (yet powerful) data structures in computer science\\nand have applicability in a wide range of scenarios. Simply put, a queue defines a\\ncollection of items that are processed in a first-in-first-out fashion. In other words, an\\nelement A that was put into the queue before element B, will be processed prior to\\nelement B. Going forward, when talking about inserting/extracting elements from a\\nqueue it is customary to say \"push\" when elements are inserted into the queue, and\\n\"pop\" when elements are extracted from the queue.\\n8', metadata={'source': './assets/elixir_patterns.pdf', 'page': 11})]"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[10: 12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a258192e-ff21-4355-9fbc-ef7c5e1e33b7",
   "metadata": {},
   "source": [
    "#### Add section on `load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6379076-2ba7-42ff-9b9b-710d525db637",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a970574-27ed-4f84-8d13-4c66cf96ec41",
   "metadata": {},
   "source": [
    "#### Do `RetrievalChain` and `ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5276eb-fbff-4f41-a874-558466674150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f440fb4-8fd4-4c28-ab59-f8066ab19979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73e973d-40a7-4dc1-9be6-9c60466d9137",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3cd3ac8-29cb-4433-88a5-9647f6554fcd",
   "metadata": {},
   "source": [
    "## Semantic Search via LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c71b77-5cac-4731-9630-c93492845805",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7b2453b-70ca-4f58-884d-6aea1aaaeefa",
   "metadata": {},
   "source": [
    "## Query csv/excel data via LangChain (plus LlamaIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389413a3-3bf5-46b1-ad6a-74c760776208",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3e355e-0801-4508-ac97-5f991b709655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fbf0ce1-cbf0-4284-bbc2-1e56cf4c014d",
   "metadata": {},
   "source": [
    "## Introduction to Agents and Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16d189e-af75-4887-97f5-ce67fb6f431b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
